# -*- coding: utf-8 -*-
"""evalutaion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BhfQjPqmCtctBhgtVqYRwcmrI9F31FaL
"""

!pip install transformers torch evaluate rouge-score nltk

!pip install evaluate rouge-score nltk

models = {
    "GPT2": "gpt2",
    "DistilGPT2": "distilgpt2",
    "T5-small": "t5-small",
    "BART-small": "facebook/bart-base"
}

prompts = [
    "Explain TOPSIS in simple words.",
    "What is machine learning?",
    "Write a short story about AI."
]

references = {
    "Explain TOPSIS in simple words.": "TOPSIS is a multi-criteria decision making method that selects the best option by comparing distances from ideal and worst solutions.",
    "What is machine learning?": "Machine learning is a field of artificial intelligence where systems learn patterns from data without explicit programming.",
    "Write a short story about AI.": "Once upon a time, an intelligent machine helped humans solve problems and improve life, learning more every day and becoming a trusted companion."
}

import evaluate

bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

results = []

for name, model_id in models.items():
    print("Loading:", name)
    gen = pipeline("text-generation", model=model_id)

    bleu_scores = []
    rouge_scores = []
    total_time = 0
    total_len = 0

    for p in prompts:
        start = time.time()
        out = gen(p, max_length=150)
        end = time.time()

        gen_text = out[0]["generated_text"]
        ref_text = references[p]

        total_time += (end - start)
        total_len += len(gen_text.split())

        bleu_score = bleu.compute(predictions=[gen_text], references=[ref_text])["bleu"]
        rouge_score = rouge.compute(predictions=[gen_text], references=[ref_text])["rougeL"]

        bleu_scores.append(bleu_score)
        rouge_scores.append(rouge_score)

    # ðŸ‘‡ AVERAGE PER MODEL
    avg_bleu = sum(bleu_scores) / len(bleu_scores)
    avg_rouge = sum(rouge_scores) / len(rouge_scores)
    avg_time = total_time / len(prompts)
    avg_len = total_len / len(prompts)

    # ðŸ‘‡ ONE ROW PER MODEL
    results.append([name, avg_bleu, avg_rouge, avg_time, avg_len])

df = pd.DataFrame(
    results,
    columns=["Model", "BLEU", "ROUGE", "Time", "Length"]
)

df.to_csv("raw_results.csv", index=False)
df

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("final_result.csv")

plt.figure(figsize=(8,5))
plt.bar(df["Model"], df["Topsis Score"])
plt.title("TOPSIS Score Comparison of Text Generation Models")
plt.xlabel("Model")
plt.ylabel("Topsis Score")
plt.tight_layout()

# Save image
plt.savefig("topsis_score_comparison.png", dpi=200)
plt.show()

plt.figure(figsize=(8,5))
plt.bar(df["Model"], df["Time"])
plt.title("Generation Time Comparison")
plt.ylabel("Time (seconds)")
plt.xlabel("Model")
plt.tight_layout()
plt.savefig("time_comparison.png", dpi=200)
plt.show()

plt.figure(figsize=(8,5))
plt.bar(df["Model"], df["ROUGE"])
plt.title("ROUGE Score Comparison")
plt.ylabel("ROUGE Score")
plt.xlabel("Model")
plt.tight_layout()
plt.savefig("rouge_comparison.png", dpi=200)
plt.show()

import pandas as pd

raw = pd.read_csv("raw_results.csv")
final = pd.read_csv("final_result.csv")

print("Raw Results (Before TOPSIS):")
display(raw)

print("Final Results (After TOPSIS):")
display(final)

