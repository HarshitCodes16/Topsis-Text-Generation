# ğŸ† TOPSIS for Selecting Best Pre-trained Text Generation Model

## ğŸ“Œ Assignment

Apply **TOPSIS** to find the **best pre-trained model** for:

- Text Summarization (Roll Numbers ending with 0 or 5)  
- âœ… **Text Generation (Roll Numbers ending with 1 or 6)**  
- Text Classification (Roll Numbers ending with 2 or 7)  
- Text Sentence Similarity (Roll Numbers ending with 3 or 8)  
- Text Conversational (Roll Numbers ending with 4 or 9)  

> My roll number ends with **6**, so this project focuses on **Text Generation**.

---

## ğŸ“– Problem Statement

The goal of this project is to **select the best pre-trained text generation model** using the **TOPSIS (Technique for Order Preference by Similarity to Ideal Solution)** multi-criteria decision-making method.

We compare multiple pre-trained models based on the following criteria:

- **BLEU Score** (higher is better)
- **ROUGE Score** (higher is better)
- **Generation Time** (lower is better)
- **Output Length** (lower is better)

---

## ğŸ“ Repository Structure

```
Topsis-Text-Generation/
â”œâ”€â”€ evaluation(2).ipynb
â”œâ”€â”€ topsis_part1.py
â”œâ”€â”€ raw_results.csv
â”œâ”€â”€ final_result.csv
â”œâ”€â”€ topsis_score_comparison.png
â”œâ”€â”€ time_comparison.png
â”œâ”€â”€ rouge_comparison.png
â””â”€â”€ README.md
```


---

## ğŸ¤– Models Compared

- GPT2  
- DistilGPT2  
- T5-small  
- BART-small  

---

## âš™ï¸ Installation

Install all required libraries using:

```bash
pip install torch transformers nltk rouge-score pandas numpy matplotlib scikit-learn


## ğŸ§ª Overall Workflow

1. **Run the evaluation notebook**

   Open and run:
   evaluation(2).ipynb
   This will:
    - Run all models on the same prompts
    - Compute:
      - BLEU
      - ROUGE
      - Time taken
      - Output length
    - Save the results into:
       raw_results.csv

 2. **Apply TOPSIS using command line**

After `raw_results.csv` is generated, open terminal in the project folder and run TOPSIS.

### ğŸ”¹ Command Format
python topsis_part1.py <InputCSV> <Weights> <Impacts> <OutputCSV>


### ğŸ”¹ Meaning of parameters

- **InputCSV** â†’ Input file (`raw_results.csv`)
- **Weights** â†’ Importance of each column (example: `"1,1,1,1"`)
- **Impacts** â†’  
  `+` means higher is better, `-` means lower is better  

  For this project:
  - BLEU â†’ +
  - ROUGE â†’ +
  - Time â†’ -
  - Length â†’ -

  So impacts:

  ```
  "+,+,-,-"
  ```

- **OutputCSV** â†’ Output file name (`final_result.csv`)

### â–¶ï¸ Actual Command Used

python topsis_part1.py raw_results.csv "1,1,1,1" "+,+,-,-" final_result.csv
This will create:
final_result.csv

which contains:
- TOPSIS Score
- Rank of each model

---

3. **Visualize and analyze results**

Using `final_result.csv`, graphs are generated:

- TOPSIS Score Comparison
- Time Comparison
- ROUGE Comparison

And the **best model is selected based on TOPSIS rank**.

---

---



## ğŸ“Š Table 1: Raw Results (Before TOPSIS)

This table contains the **actual measured performance** of each model:

ğŸ“„ File: `raw_results.csv`

---

## ğŸ† Table 2: Final Results (After TOPSIS)

This table contains:

- Normalized scores
- TOPSIS Score
- Final Rank

ğŸ“„ File: `final_result.csv`

---

## ğŸ“ˆ Graphs and Visualizations

### 1ï¸âƒ£ TOPSIS Score Comparison

![TOPSIS Score Comparison](topsis_score_comparison.png)

---

### 2ï¸âƒ£ Time Comparison

![Time Comparison](time_comparison.png)

---

### 3ï¸âƒ£ ROUGE Score Comparison

![ROUGE Comparison](rouge_comparison.png)

---

## ğŸ¥‡ Final Ranking (Based on TOPSIS)

| Rank | Model |
|------|--------|
| 1ï¸âƒ£ | **T5-small** |
| 2ï¸âƒ£ | GPT2 |
| 3ï¸âƒ£ | DistilGPT2 |
| 4ï¸âƒ£ | BART-small |

---

## ğŸ§  Conclusion

Based on the TOPSIS evaluation using multiple criteria (BLEU, ROUGE, Time, and Length), **T5-small** achieved the highest TOPSIS score and is therefore selected as the **best overall pre-trained text generation model** among the evaluated models.


ğŸ‘¨â€ğŸ’» Author

Harshit Katyal
